---
title: 微博点赞数等信息和公众号点赞数等信息爬取
top: false
comments: true
categories:
  - 爬虫
tags:
  - python
abbrlink: cd5b1c33
date: 2020-05-12 09:10:20
---

## 一：微博

<!--more-->

因为GitHub页面中已有非常详细的描述，这里只贴出地址和运行结果。

地址： https://github.com/dataabc/weiboSpider 

运行结果：（获取点赞数，转发数，评论数）

![](http://photo.jomeswang.top/20200512091954.png)

## 二：微信公众号

github地址： https://github.com/wnma3mz/wechat_articles_spider 

先看结果图：（获取url和阅读数和点赞数）![](http://photo.jomeswang.top/20200513125714.png)

### 2.1 原理介绍

####  2.1.1 抓取文章url

1. 拥有一个微信个人订阅号，附上登陆和注册链接。[微信公众平台](https://mp.weixin.qq.com/)
2. 登陆之后，点击左侧菜单栏“管理”-“素材管理”。再点击右边的“新建图文素材”

![](http://photo.jomeswang.top/20200513134801.png)

1. 弹出一个新的标签页，在上面的工具栏找到“超链接”并点击
   ![](http://photo.jomeswang.top/20200513135008.png)

2. 弹出了一个小窗口，选择“查找文章”，输入需要查找的公众号，这里用“科技美学”公众号作为例子
   ![](http://photo.jomeswang.top/20200513135022.png)

3. 点击之后，可以弹出该公众号的所有历史文章

   ![](http://photo.jomeswang.top/20200513135037.png)

#### 2.1.2 抓取文章的阅读量和点赞量

1.首先安装Fiddler抓包工具软件

下载链接：https://www.telerik.com/download/fiddler/fiddler4

安装完成，只需点击同意，下一步安装即可。

2.Fiddler相关配置

安装完毕后，按下图图进行配置勾选，然后重启Fiddler。

![](http://photo.jomeswang.top/20200513182844.png)

![](http://photo.jomeswang.top/20200513170351.png)

![](http://photo.jomeswang.top/20200513170406.png)

按图进行配置操作，然后重启Fiddler。

3.  登陆微信客户端，浏览该公众号的任意一篇推文 

4.  可以观察到这里的内容显示会有阅读量、点赞量、评论等 

5.  观察fiddler的监控数据，如下图显示 

   ![](http://photo.jomeswang.top/20200513170518.png)

6.  其中`/mp/getappmgsext?...`是我们推文内容的url，双击之后，fiddler界面右边出现如下图数据 

   ![](http://photo.jomeswang.top/20200513171219.png)

### 2.2 用法

#### 2.2.1 克隆项目并安装依赖

```
git clone https://github.com/wnma3mz/wechat_articles_spider.git

# 在项目文件夹内
pip install -r requirements.txt
```

#### 2.2.2 获取微信公众号token和cookie

1. 登录微信公众号， 打开浏览器的**开发者选项**(F12), 推荐Chrome或者Firefox 

   ![](http://photo.jomeswang.top/20200513171719.png)

#### 2.2.2 获取个人微信号的token和cookie（要打开想要爬的公众号所属文章）

1. 打开fiddler开始监控
2. 登陆微信PC端，浏览该公众号的任意一篇推文

3.  其中`/mp/getappmgsext?...`是我们推文内容的url，双击之后，fiddler界面右边出现如下图数据 

4. 获取token

   ![](http://photo.jomeswang.top/20200513172115.png)

5. 获取cookie 

   ![](http://photo.jomeswang.top/20200513172224.png)

#### 2.2.3 测试cookie和token(填入相关信息)

![](http://photo.jomeswang.top/20200513181352.png)

方法一（一次性检测）：

```
# coding=utf-8

import time
from wechatarticles import ArticlesInfo, ArticlesUrls, tools
import pandas as pd
import sys
import os
sys.path.append(os.getcwd())


# 公众号的cookie和token
official_cookie = ""
token = ""
# 个人微信号的cookie和token
appmsg_token, wechat_cookie = "", ""

#  查询的公众号名称
nickname = ""

# 实例化爬取对象
# 手动输入账号密码
test_url = ArticlesUrls(cookie=official_cookie, token=token)
test_info = ArticlesInfo(appmsg_token=appmsg_token, cookie=wechat_cookie)


def get_info(articles_data, test_info):
    articles = []
    try:
        # print(articles_data)
        for article_data in articles_data:
            print(article_data)
            # 设置一定的间隔时间防止被禁最好是 10s左右
            time.sleep(6)
            # print(article_data)
            time_array = time.localtime(int(article_data['create_time']))
            datetime = time.strftime("%Y-%m-%d %H:%M:%S", time_array)
            year = time.strftime("%Y", time_array)
            read_num, like_num = test_info.read_like_nums(article_data['link'])
            article = {"title": article_data['title'], 'digest': article_data['digest'],
                       'link': article_data['link'], 'time': datetime, 'year': year,
                       'read_num': read_num, 'like_num': like_num}
            articles.append(article)
    except Exception as error:
        print(articles_data)
        print(error)
    finally:
        # 在遇到意外时先将已经弄出来的文章URL保存至 test.csv 中
        with open("./test2.txt", "a+", encoding="utf-8") as f:
            f.write(str(articles_data))
            f.write("\n")
            f.write("\n")

        # 在遇到意外时先将已经弄出来的文章阅读数和点赞数保存至 test.csv 中
        a = pd.DataFrame(articles)
        # print(a)
        a.to_csv('./test2.csv')
        print("写入成功")


def main():

    # 输入公众号名称，获取公众号文章总数
    articles_sum = test_url.articles_nums(nickname)
    count = 5
    begin = 0
    articles_data = []
    articles = []
    while True:
        try:
            # 设置一个间隔时间，亲测这个时间可以跑两三百条数据，但时间也可以更长一点
            time.sleep(60)
            print(begin)
            # 获取文章数据（如url等）大于五条
            temp = test_url.articles(nickname, begin=str(begin), count=str(count))
            # 重复获取所有的文章的url，并储存到一个列表中
            articles_data += temp
            print('----{}----{}'.format(len(temp), len(articles_data)))

        except Exception as e:
            print("Working Error!")
            print(e)
            # 当公众号被禁时进入获取文章点赞数函数进去
            get_info(articles_data, test_info)
            break
        begin += count
        if begin > 10:
            print('End!!')
            # 当得到数据时进入获取文章点赞数函数进去
            get_info(articles_data, test_info)
            break

    print("第{}篇文章爬取失败，请过段时间再次尝试或换个帐号继续爬取".format(begin))
    print("Success!!!!")


if __name__ == "__main__":

    main()

```

方法二（分部检测）：

1. 在项目文件夹中有个test文件夹进入后

   ![](http://photo.jomeswang.top/20200513173431.png)

### 2.3 遇到的Bug

1. 由于打开的文章获取的token和要去爬的公众号不属于同一公众号，于是会报这个错误。所以要重新打开相同的公众号文章获取token和cookie。

![](http://photo.jomeswang.top/20200513180206.png)

2. ![](http://photo.jomeswang.top/20200513181103.png)

